{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da542328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa97993",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode, step = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f49357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Q-Network for DQN Agent\n",
    "# -----------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions, grid_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),  # [B, 32, H, W]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),              # [B, 64, H, W]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),              # [B, 64, H, W]\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Flatten after conv layers\n",
    "        self.fc_input_size = None  # Placeholder, will be set dynamically\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * grid_size * grid_size, 512),  # Update based on actual grid size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prioritized Replay Buffer\n",
    "# -----------------------------\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity (int): Maximum number of transitions to store.\n",
    "            alpha (float): How much prioritization is used \n",
    "                           (0 = no prioritization, 1 = full prioritization).\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []             # List to store experiences.\n",
    "        self.priorities = []         # List to store priorities.\n",
    "        self.alpha = alpha\n",
    "        self.pos = 0\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience to the buffer with maximum priority.\"\"\"\n",
    "        # If the buffer is not full, append the new experience;\n",
    "        # otherwise, replace the oldest one (circular buffer).\n",
    "        max_priority = max(self.priorities) if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "            self.priorities.append(max_priority)\n",
    "        else:\n",
    "            self.buffer[self.pos] = experience\n",
    "            self.priorities[self.pos] = max_priority\n",
    "            self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"\n",
    "        Samples a batch of experiences with probabilities proportional to their priorities.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of samples to draw.\n",
    "            beta (float): Importance-sampling, from initial value increasing to 1.\n",
    "        \n",
    "        Returns:\n",
    "            samples: List of sampled experiences.\n",
    "            indices: The indices of the sampled experiences.\n",
    "            weights: Importance sampling weights for the batch.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        prios = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = prios ** self.alpha\n",
    "        probs_sum = probs.sum()\n",
    "        if probs_sum == 0 or np.isnan(probs_sum):\n",
    "            probs = np.ones_like(probs) / len(probs)\n",
    "        else:\n",
    "            probs /= probs_sum\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()  # Normalize\n",
    "\n",
    "        return samples, indices, weights\n",
    "    \n",
    "\n",
    "    def update_priorities(self, indices, new_priorities):\n",
    "        \"\"\"\n",
    "        Updates the priorities of sampled experiences.\n",
    "        \n",
    "        Args:\n",
    "            indices (list of int): The indices of the experiences to update.\n",
    "            new_priorities (list of float): The new priority for each corresponding experience.\n",
    "        \"\"\"\n",
    "        for idx, priority in zip(indices, new_priorities):\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump((self.buffer, self.priorities, self.pos), f)\n",
    "        print(f\"Replay buffer saved to {filepath}\")\n",
    "\n",
    "    def load(self, filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            self.buffer, self.priorities, self.pos = pickle.load(f)\n",
    "        print(f\"Replay buffer loaded from {filepath}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DQN Agent\n",
    "# -----------------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_space, state_space, grid_size, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.1, \n",
    "                 epsilon_decay=0.995, batch_size=64, buffer_size=10000):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.step = 0  # <--- Track steps here\n",
    "\n",
    "        input_channels = 3  # Agent, rewards, enemies\n",
    "\n",
    "        self.q_network = QNetwork(input_channels, action_space, grid_size).to(self.device)\n",
    "        self.target_network = QNetwork(input_channels, action_space, grid_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.buffer = PrioritizedReplayBuffer(buffer_size)\n",
    "\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        \n",
    "        # Convert to shape [1, C, H, W] assuming 3-channel input (C=3)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # [1, 3, H, W]\n",
    "        q_values = self.q_network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, beta=0.4):\n",
    "        if self.buffer.size() < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch, indices, weights = self.buffer.sample(self.batch_size, beta)\n",
    "        if not batch:\n",
    "            return  # Safety check\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert batch to tensors\n",
    "        # Shape: [B, 3, H, W] assuming 3-channel grid input\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)  # [B, 3, H, W]\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)  # [B, 3, H, W]\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "\n",
    "        # Forward pass\n",
    "        q_values = self.q_network(states).gather(1, actions)  # [B, 1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1, keepdim=True)[0]  # [B, 1]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        td_errors = q_values - target_q_values\n",
    "        loss = (weights * td_errors.pow(2)).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update priorities\n",
    "        new_priorities = td_errors.abs().detach().cpu().numpy().flatten() + 1e-6\n",
    "        self.buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, filepath, episode, step):\n",
    "        torch.save({\n",
    "            'policy_net_state_dict': self.q_network.state_dict(),\n",
    "            'target_net_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'episode': episode,\n",
    "            'step': step,\n",
    "            'epsilon': self.epsilon\n",
    "        }, filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        episode = checkpoint.get('episode', 0)\n",
    "        step = checkpoint.get('step', 0)\n",
    "        self.epsilon = checkpoint.get('epsilon', 1.0)\n",
    "        print(f\"Loaded model from {filepath} | episode: {episode} | step: {step} | epsilon: {self.epsilon:.4f}\")\n",
    "        return episode, step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0069e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Collect & Avoid Environment (gym.Env subclass)\n",
    "# -------------------------------------------\n",
    "\n",
    "class CollectAvoidEnv(gym.Env):\n",
    "    def __init__(self, grid_size=15, num_rewards=5, num_enemies=3, enemy_random_move_ratio=0.5, max_steps=1000):\n",
    "        super(CollectAvoidEnv, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.num_rewards = num_rewards\n",
    "        self.num_enemies = num_enemies\n",
    "        self.enemy_random_move_ratio = enemy_random_move_ratio\n",
    "        self.reward_positions = []\n",
    "        self.enemy_positions = []\n",
    "        self.agent_pos = None\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Action space: 5 discrete actions\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        # Observation space: 3-channel grid (agent, rewards, enemies)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=(3, self.grid_size, self.grid_size),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Plotting setup\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 6))\n",
    "        self.ax.set_xlim(0, self.grid_size - 1)\n",
    "        self.ax.set_ylim(0, self.grid_size - 1)\n",
    "        self.ax.set_xticks(range(self.grid_size))\n",
    "        self.ax.set_yticks(range(self.grid_size))\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        from math import sqrt\n",
    "\n",
    "        # Reset the step counter\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Randomly place rewards on the grid, avoiding overlap\n",
    "        self.reward_positions = [self._random_empty_cell([]) for _ in range(self.num_rewards)]\n",
    "\n",
    "        # Randomly place the agent on the grid, avoiding rewards\n",
    "        self.agent_pos = self._random_empty_cell(self.reward_positions)\n",
    "\n",
    "        # Calculate the minimum required distance between agent and each enemy\n",
    "        # 0.6 * grid_size * sqrt(2) is roughly 60% of the max diagonal distance\n",
    "        min_dist = 0.6 * self.grid_size * sqrt(2)\n",
    "\n",
    "        self.enemy_positions = []\n",
    "        for _ in range(self.num_enemies):\n",
    "            while True:\n",
    "                # Generate a candidate position for the enemy, avoiding overlaps\n",
    "                candidate = self._random_empty_cell(\n",
    "                    self.reward_positions + self.enemy_positions + [self.agent_pos]\n",
    "                )\n",
    "\n",
    "                # Compute Euclidean distance from the agent\n",
    "                dist = sqrt((candidate[0] - self.agent_pos[0])**2 +\n",
    "                            (candidate[1] - self.agent_pos[1])**2)\n",
    "\n",
    "                # Accept candidate only if it's far enough from the agent\n",
    "                if dist >= min_dist:\n",
    "                    self.enemy_positions.append(candidate)\n",
    "                    break\n",
    "\n",
    "        # Return the initial observation/state\n",
    "        return self._get_state()\n",
    "\n",
    "\n",
    "    def step(self, action, episode, step):\n",
    "        self.current_step += 1\n",
    "        prev_agent_pos = self.agent_pos\n",
    "\n",
    "        # Move agent\n",
    "        if action == 0:  # stay\n",
    "            new_pos = self.agent_pos\n",
    "        elif action == 1:  # up\n",
    "            new_pos = (max(self.agent_pos[0] - 1, 0), self.agent_pos[1])\n",
    "        elif action == 2:  # down\n",
    "            new_pos = (min(self.agent_pos[0] + 1, self.grid_size - 1), self.agent_pos[1])\n",
    "        elif action == 3:  # left\n",
    "            new_pos = (self.agent_pos[0], max(self.agent_pos[1] - 1, 0))\n",
    "        elif action == 4:  # right\n",
    "            new_pos = (self.agent_pos[0], min(self.agent_pos[1] + 1, self.grid_size - 1))\n",
    "\n",
    "        self.agent_pos = new_pos\n",
    "\n",
    "        # Move enemies\n",
    "        self._move_enemies()\n",
    "\n",
    "        # Compute reward and check if agent is caught by an enemy or has collected all rewards\n",
    "        reward = self._compute_reward(prev_agent_pos)\n",
    "        done = self.agent_pos in self.enemy_positions or len(self.reward_positions) == 0 or self.current_step >= self.max_steps # Terminate if caught by enemy or all rewards are collected\n",
    "\n",
    "        # Render environment\n",
    "        self.render(episode, step, reward)\n",
    "        return self._get_state(), reward, done, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        state = np.zeros((3, self.grid_size, self.grid_size), dtype=np.float32)\n",
    "\n",
    "        # Agent in channel 0\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "\n",
    "        # Rewards in channel 1\n",
    "        for r in self.reward_positions:\n",
    "            state[1, r[0], r[1]] = 1.0\n",
    "\n",
    "        # Enemies in channel 2\n",
    "        for e in self.enemy_positions:\n",
    "            state[2, e[0], e[1]] = 1.0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _compute_reward(self, prev_agent_pos):\n",
    "        reward = 0.0\n",
    "\n",
    "        if self.agent_pos in self.reward_positions:\n",
    "            self.reward_positions.remove(self.agent_pos)\n",
    "            reward += 3.0\n",
    "\n",
    "        # Reward for being near remaining rewards\n",
    "        for rx, ry in self.reward_positions:\n",
    "            dist = abs(self.agent_pos[0] - rx) + abs(self.agent_pos[1] - ry)\n",
    "            if dist == 1: reward += 0.2\n",
    "            elif dist == 2: reward += 0.15\n",
    "            elif dist == 3: reward += 0.1\n",
    "\n",
    "        # Penalty for being near enemies\n",
    "        for ex, ey in self.enemy_positions:\n",
    "            edist = abs(self.agent_pos[0] - ex) + abs(self.agent_pos[1] - ey)\n",
    "            if edist == 1: reward -= 1\n",
    "            elif edist == 2: reward -= 0.5\n",
    "            elif edist == 3: reward -= 0.3\n",
    "            elif edist == 4: reward -= 0.2\n",
    "\n",
    "        # Reward for increasing distance from enemies\n",
    "        if self.enemy_positions:\n",
    "            prev_avg = np.mean([abs(prev_agent_pos[0] - ex) + abs(prev_agent_pos[1] - ey) for ex, ey in self.enemy_positions])\n",
    "            curr_avg = np.mean([abs(self.agent_pos[0] - ex) + abs(self.agent_pos[1] - ey) for ex, ey in self.enemy_positions])\n",
    "            if curr_avg > prev_avg: reward += 0.3\n",
    "            elif curr_avg < prev_avg: reward -= 0.3\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _move_enemies(self):\n",
    "        for i in range(len(self.enemy_positions)):\n",
    "            x, y = self.enemy_positions[i]\n",
    "            ax, ay = self.agent_pos\n",
    "\n",
    "            if random.random() < (1 - self.enemy_random_move_ratio):\n",
    "                if ax > x: x += 1\n",
    "                elif ax < x: x -= 1\n",
    "                if ay > y: y += 1\n",
    "                elif ay < y: y -= 1\n",
    "            else:\n",
    "                dx, dy = random.choice([(0, 1), (1, 0), (0, -1), (-1, 0)])\n",
    "                x = max(0, min(self.grid_size - 1, x + dx))\n",
    "                y = max(0, min(self.grid_size - 1, y + dy))\n",
    "\n",
    "            self.enemy_positions[i] = (x, y)\n",
    "\n",
    "    def _random_empty_cell(self, excluded_cells):\n",
    "        while True:\n",
    "            cell = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n",
    "            if cell not in excluded_cells:\n",
    "                return cell\n",
    "\n",
    "    def render(self, episode, step, reward=None):\n",
    "        self.ax.clear()\n",
    "        self.ax.set_xlim(0, self.grid_size - 1)\n",
    "        self.ax.set_ylim(0, self.grid_size - 1)\n",
    "        self.ax.set_xticks(range(self.grid_size))\n",
    "        self.ax.set_yticks(range(self.grid_size))\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        self.ax.plot(self.agent_pos[0], self.agent_pos[1], 'bo', markersize=10)\n",
    "        for r_pos in self.reward_positions:\n",
    "            self.ax.plot(r_pos[0], r_pos[1], 'go', markersize=10)\n",
    "        for e_pos in self.enemy_positions:\n",
    "            self.ax.plot(e_pos[0], e_pos[1], 'ro', markersize=10)\n",
    "\n",
    "        self.ax.text(0.5, self.grid_size - 1, f'Episode: {episode}, Step: {step}',\n",
    "                     horizontalalignment='center', verticalalignment='top', fontsize=12, color='black', weight='bold')\n",
    "\n",
    "        if reward is not None:\n",
    "            reward_color = 'green' if reward > 0 else 'red' if reward < 0 else 'black'\n",
    "            self.ax.text(0.5, self.grid_size - 2, f'Reward: {reward:.2f}',\n",
    "                         horizontalalignment='center', verticalalignment='top', fontsize=12, color=reward_color, weight='bold')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e20c06",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     50\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo saved buffer found, starting with empty buffer.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_episode, NUMBER_OF_EPISODES):\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     state = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     56\u001b[39m     step = \u001b[32m0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mCollectAvoidEnv.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.enemy_positions = []\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_enemies):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     58\u001b[39m         \u001b[38;5;66;03m# Generate a candidate position for the enemy, avoiding overlaps\u001b[39;00m\n\u001b[32m     59\u001b[39m         candidate = \u001b[38;5;28mself\u001b[39m._random_empty_cell(\n\u001b[32m     60\u001b[39m             \u001b[38;5;28mself\u001b[39m.reward_positions + \u001b[38;5;28mself\u001b[39m.enemy_positions + [\u001b[38;5;28mself\u001b[39m.agent_pos]\n\u001b[32m     61\u001b[39m         )\n\u001b[32m     63\u001b[39m         \u001b[38;5;66;03m# Compute Euclidean distance from the agent\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# Train the Agent\n",
    "# -------------------------------------------\n",
    "\n",
    "\"\"\" Input Variables \"\"\"\n",
    "GRID_SIZE = 30\n",
    "NUM_REWARDS = 12\n",
    "NUM_ENEMIES = 1\n",
    "ENEMY_RANDOM_MOVE_RATIO = 0.6\n",
    "NUMBER_OF_EPISODES = 10000\n",
    "MAX_STEPS_PER_EPISODE= 700\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.95  # 0: only immediate reward matters ; 1.0: future rewards are just as important as immediate ones.\n",
    "EPSILON = 1.0   # initial value for weighting random over policy in taking actions\n",
    "EPSILON_MIN = 0.2\n",
    "EPSILON_DECAY = 0.9999  # multiplies random action chance with this factor after every training\n",
    "BATCH_SIZE = 8  # number of samples to take from the replay buffer for training\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Define the annealing parameters for beta (Prioritized Replay Buffer)\n",
    "BETA_START = 0.4  # Starting value for beta (usually smaller)\n",
    "BEAT_FRAMES = 10  # Number of frames after which beta will reach 1.0\n",
    "TOTAL_FRAMES = NUMBER_OF_EPISODES * 100  # Total frames in the training\n",
    "\n",
    "RESUME_TRAINING = False\n",
    "MODEL_PATH = r\"E:\\Git_repos\\RL_playground\\CollectAndAvoid\\trained_models\\cnn_models\\dqn_cnn_model_1.pth\"\n",
    "BUFFER_PATH = r\"E:\\Git_repos\\RL_playground\\CollectAndAvoid\\trained_models\\cnn_models\\buffer_CNN_1.pth\"\n",
    "\"\"\" END of Input Variables \"\"\"\n",
    "\n",
    "env = CollectAvoidEnv(grid_size=GRID_SIZE, num_rewards=NUM_REWARDS, num_enemies=NUM_ENEMIES, enemy_random_move_ratio=ENEMY_RANDOM_MOVE_RATIO, max_steps=MAX_STEPS_PER_EPISODE)\n",
    "agent = DQNAgent(env.action_space.n, np.prod(env.observation_space.shape), GRID_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON,\n",
    "                 epsilon_min=EPSILON_MIN, epsilon_decay=EPSILON_DECAY, batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE)\n",
    "\n",
    "start_episode = 0\n",
    "start_step = 0\n",
    "\n",
    "# Try loading an existing model\n",
    "if RESUME_TRAINING:\n",
    "    try:\n",
    "         start_episode, start_step = agent.load(MODEL_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved model found, starting from scratch.\")\n",
    "\n",
    "# Try loading an existing buffer\n",
    "if RESUME_TRAINING:\n",
    "    try:\n",
    "        agent.buffer.load(BUFFER_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved buffer found, starting with empty buffer.\")\n",
    "\n",
    "\n",
    "for episode in range(start_episode, NUMBER_OF_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    # Calculate beta for the current training step (frame_idx)\n",
    "    frame_idx = episode * 50 + step  # Adjust this according to your setup\n",
    "    beta = min(1.0, BETA_START + (BEAT_FRAMES - BETA_START) * frame_idx / TOTAL_FRAMES)\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)  # Get action from your agent\n",
    "        next_state, reward, done, _ = env.step(action, episode, step)\n",
    "        step += 1\n",
    "        agent.buffer.add((state, action, reward, next_state, float(done)))\n",
    "\n",
    "        print(f\"E{episode} S{step} | reward: {reward:.3f} | epsilon: {agent.epsilon:.3f} | beta: {beta:.3f}\")\n",
    "\n",
    "        agent.train(beta=beta)\n",
    "        # if episode > 2000:\n",
    "        #     time.sleep(0.1)\n",
    "        \n",
    "    # Every 10 episodes, update target network and save model and buffer\n",
    "    if episode % 10 == 0:\n",
    "        agent.update_target_network()\n",
    "        agent.save(MODEL_PATH, episode, step)\n",
    "        agent.buffer.save(BUFFER_PATH)\n",
    "        print(\"Model and Buffer saved.\")\n",
    "        \n",
    "    print(f\"Episode {episode + 1} finished\")\n",
    "\n",
    "# Save the final model and buffer after training is complete\n",
    "agent.save(MODEL_PATH, episode, step)\n",
    "agent.buffer.save(BUFFER_PATH)\n",
    "print(\"Training complete, model and buffer saved.\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b0d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
