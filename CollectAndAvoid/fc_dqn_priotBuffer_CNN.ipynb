{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da542328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4fa97993",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode, step = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f49357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Q-Network for DQN Agent\n",
    "# -----------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),  # [B, 32, H, W]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),              # [B, 64, H, W]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),              # [B, 64, H, W]\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Flatten after conv layers\n",
    "        self.fc_input_size = None  # Placeholder, will be set dynamically\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 15 * 15, 512),  # Update based on actual grid size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prioritized Replay Buffer\n",
    "# -----------------------------\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity (int): Maximum number of transitions to store.\n",
    "            alpha (float): How much prioritization is used \n",
    "                           (0 = no prioritization, 1 = full prioritization).\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []             # List to store experiences.\n",
    "        self.priorities = []         # List to store priorities.\n",
    "        self.alpha = alpha\n",
    "        self.pos = 0\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience to the buffer with maximum priority.\"\"\"\n",
    "        # If the buffer is not full, append the new experience;\n",
    "        # otherwise, replace the oldest one (circular buffer).\n",
    "        max_priority = max(self.priorities) if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "            self.priorities.append(max_priority)\n",
    "        else:\n",
    "            self.buffer[self.pos] = experience\n",
    "            self.priorities[self.pos] = max_priority\n",
    "            self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"\n",
    "        Samples a batch of experiences with probabilities proportional to their priorities.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of samples to draw.\n",
    "            beta (float): Importance-sampling, from initial value increasing to 1.\n",
    "        \n",
    "        Returns:\n",
    "            samples: List of sampled experiences.\n",
    "            indices: The indices of the sampled experiences.\n",
    "            weights: Importance sampling weights for the batch.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        prios = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = prios ** self.alpha\n",
    "        probs_sum = probs.sum()\n",
    "        if probs_sum == 0 or np.isnan(probs_sum):\n",
    "            probs = np.ones_like(probs) / len(probs)\n",
    "        else:\n",
    "            probs /= probs_sum\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()  # Normalize\n",
    "\n",
    "        return samples, indices, weights\n",
    "    \n",
    "\n",
    "    def update_priorities(self, indices, new_priorities):\n",
    "        \"\"\"\n",
    "        Updates the priorities of sampled experiences.\n",
    "        \n",
    "        Args:\n",
    "            indices (list of int): The indices of the experiences to update.\n",
    "            new_priorities (list of float): The new priority for each corresponding experience.\n",
    "        \"\"\"\n",
    "        for idx, priority in zip(indices, new_priorities):\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump((self.buffer, self.priorities, self.pos), f)\n",
    "        print(f\"Replay buffer saved to {filepath}\")\n",
    "\n",
    "    def load(self, filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            self.buffer, self.priorities, self.pos = pickle.load(f)\n",
    "        print(f\"Replay buffer loaded from {filepath}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DQN Agent\n",
    "# -----------------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_space, state_space, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.1, \n",
    "                 epsilon_decay=0.995, batch_size=64, buffer_size=10000):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.step = 0  # <--- Track steps here\n",
    "\n",
    "        input_channels = 3  # Agent, rewards, enemies\n",
    "\n",
    "        self.q_network = QNetwork(input_channels, action_space).to(self.device)\n",
    "        self.target_network = QNetwork(input_channels, action_space).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.buffer = PrioritizedReplayBuffer(buffer_size)\n",
    "\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        \n",
    "        # Convert to shape [1, C, H, W] assuming 3-channel input (C=3)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # [1, 3, H, W]\n",
    "        q_values = self.q_network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, beta=0.4):\n",
    "        if self.buffer.size() < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch, indices, weights = self.buffer.sample(self.batch_size, beta)\n",
    "        if not batch:\n",
    "            return  # Safety check\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert batch to tensors\n",
    "        # Shape: [B, 3, H, W] assuming 3-channel grid input\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)  # [B, 3, H, W]\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)  # [B, 3, H, W]\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "\n",
    "        # Forward pass\n",
    "        q_values = self.q_network(states).gather(1, actions)  # [B, 1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1, keepdim=True)[0]  # [B, 1]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        td_errors = q_values - target_q_values\n",
    "        loss = (weights * td_errors.pow(2)).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update priorities\n",
    "        new_priorities = td_errors.abs().detach().cpu().numpy().flatten() + 1e-6\n",
    "        self.buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, filepath, episode, step):\n",
    "        torch.save({\n",
    "            'policy_net_state_dict': self.q_network.state_dict(),\n",
    "            'target_net_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'episode': episode,\n",
    "            'step': step,\n",
    "            'epsilon': self.epsilon\n",
    "        }, filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        episode = checkpoint.get('episode', 0)\n",
    "        step = checkpoint.get('step', 0)\n",
    "        self.epsilon = checkpoint.get('epsilon', 1.0)\n",
    "        print(f\"Loaded model from {filepath} | episode: {episode} | step: {step} | epsilon: {self.epsilon:.4f}\")\n",
    "        return episode, step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0069e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Collect & Avoid Environment (gym.Env subclass)\n",
    "# -------------------------------------------\n",
    "\n",
    "class CollectAvoidEnv(gym.Env):\n",
    "    def __init__(self, grid_size=15, num_rewards=5, num_enemies=3, enemy_random_move_ratio=0.5):\n",
    "        super(CollectAvoidEnv, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.num_rewards = num_rewards\n",
    "        self.num_enemies = num_enemies\n",
    "        self.enemy_random_move_ratio = enemy_random_move_ratio\n",
    "        self.reward_positions = []\n",
    "        self.enemy_positions = []\n",
    "        self.agent_pos = None\n",
    "\n",
    "        # Action space: 5 discrete actions\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        # Observation space: 3-channel grid (agent, rewards, enemies)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=(3, self.grid_size, self.grid_size),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Plotting setup\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 6))\n",
    "        self.ax.set_xlim(0, self.grid_size - 1)\n",
    "        self.ax.set_ylim(0, self.grid_size - 1)\n",
    "        self.ax.set_xticks(range(self.grid_size))\n",
    "        self.ax.set_yticks(range(self.grid_size))\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.reward_positions = [self._random_empty_cell([]) for _ in range(self.num_rewards)]\n",
    "        self.enemy_positions = [self._random_empty_cell(self.reward_positions) for _ in range(self.num_enemies)]\n",
    "        self.agent_pos = self._random_empty_cell(self.reward_positions + self.enemy_positions)\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action, episode, step):\n",
    "        prev_agent_pos = self.agent_pos\n",
    "\n",
    "        # Move agent\n",
    "        if action == 0:  # stay\n",
    "            new_pos = self.agent_pos\n",
    "        elif action == 1:  # up\n",
    "            new_pos = (max(self.agent_pos[0] - 1, 0), self.agent_pos[1])\n",
    "        elif action == 2:  # down\n",
    "            new_pos = (min(self.agent_pos[0] + 1, self.grid_size - 1), self.agent_pos[1])\n",
    "        elif action == 3:  # left\n",
    "            new_pos = (self.agent_pos[0], max(self.agent_pos[1] - 1, 0))\n",
    "        elif action == 4:  # right\n",
    "            new_pos = (self.agent_pos[0], min(self.agent_pos[1] + 1, self.grid_size - 1))\n",
    "\n",
    "        self.agent_pos = new_pos\n",
    "\n",
    "        self._move_enemies()\n",
    "\n",
    "        reward = self._compute_reward(prev_agent_pos)\n",
    "        done = self.agent_pos in self.enemy_positions\n",
    "\n",
    "        self.render(episode, step, reward)\n",
    "        return self._get_state(), reward, done, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        state = np.zeros((3, self.grid_size, self.grid_size), dtype=np.float32)\n",
    "\n",
    "        # Agent in channel 0\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "\n",
    "        # Rewards in channel 1\n",
    "        for r in self.reward_positions:\n",
    "            state[1, r[0], r[1]] = 1.0\n",
    "\n",
    "        # Enemies in channel 2\n",
    "        for e in self.enemy_positions:\n",
    "            state[2, e[0], e[1]] = 1.0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _compute_reward(self, prev_agent_pos):\n",
    "        reward = 0.0\n",
    "\n",
    "        if self.agent_pos in self.reward_positions:\n",
    "            self.reward_positions.remove(self.agent_pos)\n",
    "            reward += 3.0\n",
    "\n",
    "        for rx, ry in self.reward_positions:\n",
    "            dist = abs(self.agent_pos[0] - rx) + abs(self.agent_pos[1] - ry)\n",
    "            if dist == 1: reward += 0.2\n",
    "            elif dist == 2: reward += 0.15\n",
    "            elif dist == 3: reward += 0.1\n",
    "\n",
    "        for ex, ey in self.enemy_positions:\n",
    "            edist = abs(self.agent_pos[0] - ex) + abs(self.agent_pos[1] - ey)\n",
    "            if edist == 1: reward -= 1\n",
    "            elif edist == 2: reward -= 0.5\n",
    "            elif edist == 3: reward -= 0.3\n",
    "            elif edist == 4: reward -= 0.2\n",
    "\n",
    "        if self.enemy_positions:\n",
    "            prev_avg = np.mean([abs(prev_agent_pos[0] - ex) + abs(prev_agent_pos[1] - ey) for ex, ey in self.enemy_positions])\n",
    "            curr_avg = np.mean([abs(self.agent_pos[0] - ex) + abs(self.agent_pos[1] - ey) for ex, ey in self.enemy_positions])\n",
    "            if curr_avg > prev_avg: reward += 0.3\n",
    "            elif curr_avg < prev_avg: reward -= 0.3\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _move_enemies(self):\n",
    "        for i in range(len(self.enemy_positions)):\n",
    "            x, y = self.enemy_positions[i]\n",
    "            ax, ay = self.agent_pos\n",
    "\n",
    "            if random.random() < (1 - self.enemy_random_move_ratio):\n",
    "                if ax > x: x += 1\n",
    "                elif ax < x: x -= 1\n",
    "                if ay > y: y += 1\n",
    "                elif ay < y: y -= 1\n",
    "            else:\n",
    "                dx, dy = random.choice([(0, 1), (1, 0), (0, -1), (-1, 0)])\n",
    "                x = max(0, min(self.grid_size - 1, x + dx))\n",
    "                y = max(0, min(self.grid_size - 1, y + dy))\n",
    "\n",
    "            self.enemy_positions[i] = (x, y)\n",
    "\n",
    "    def _random_empty_cell(self, excluded_cells):\n",
    "        while True:\n",
    "            cell = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n",
    "            if cell not in excluded_cells:\n",
    "                return cell\n",
    "\n",
    "    def render(self, episode, step, reward=None):\n",
    "        self.ax.clear()\n",
    "        self.ax.set_xlim(0, self.grid_size - 1)\n",
    "        self.ax.set_ylim(0, self.grid_size - 1)\n",
    "        self.ax.set_xticks(range(self.grid_size))\n",
    "        self.ax.set_yticks(range(self.grid_size))\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        self.ax.plot(self.agent_pos[0], self.agent_pos[1], 'bo', markersize=10)\n",
    "        for r_pos in self.reward_positions:\n",
    "            self.ax.plot(r_pos[0], r_pos[1], 'go', markersize=10)\n",
    "        for e_pos in self.enemy_positions:\n",
    "            self.ax.plot(e_pos[0], e_pos[1], 'ro', markersize=10)\n",
    "\n",
    "        self.ax.text(0.5, self.grid_size - 1, f'Episode: {episode}, Step: {step}',\n",
    "                     horizontalalignment='center', verticalalignment='top', fontsize=12, color='black', weight='bold')\n",
    "\n",
    "        if reward is not None:\n",
    "            reward_color = 'green' if reward > 0 else 'red' if reward < 0 else 'black'\n",
    "            self.ax.text(0.5, self.grid_size - 2, f'Reward: {reward:.2f}',\n",
    "                         horizontalalignment='center', verticalalignment='top', fontsize=12, color=reward_color, weight='bold')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e20c06",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     62\u001b[39m     action = agent.act(state)  \u001b[38;5;66;03m# Get action from your agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     next_state, reward, done, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     step += \u001b[32m1\u001b[39m\n\u001b[32m     65\u001b[39m     agent.buffer.add((state, action, reward, next_state, \u001b[38;5;28mfloat\u001b[39m(done)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mCollectAvoidEnv.step\u001b[39m\u001b[34m(self, action, episode, step)\u001b[39m\n\u001b[32m     62\u001b[39m reward = \u001b[38;5;28mself\u001b[39m._compute_reward(prev_agent_pos)\n\u001b[32m     63\u001b[39m done = \u001b[38;5;28mself\u001b[39m.agent_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enemy_positions\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_state(), reward, done, {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36mCollectAvoidEnv.render\u001b[39m\u001b[34m(self, episode, step, reward)\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mself\u001b[39m.ax.text(\u001b[32m0.5\u001b[39m, \u001b[38;5;28mself\u001b[39m.grid_size - \u001b[32m2\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m    155\u001b[39m                  horizontalalignment=\u001b[33m'\u001b[39m\u001b[33mcenter\u001b[39m\u001b[33m'\u001b[39m, verticalalignment=\u001b[33m'\u001b[39m\u001b[33mtop\u001b[39m\u001b[33m'\u001b[39m, fontsize=\u001b[32m12\u001b[39m, color=reward_color, weight=\u001b[33m'\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    157\u001b[39m clear_output(wait=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\IPython\\core\\display_functions.py:278\u001b[39m, in \u001b[36mdisplay\u001b[39m\u001b[34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m     publish_display_data(data=obj, metadata=metadata, **kwargs)\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     format_dict, md_dict = \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[32m    280\u001b[39m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\IPython\\core\\formatters.py:238\u001b[39m, in \u001b[36mDisplayFormatter.format\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m    236\u001b[39m md = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     data = \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\decorator.py:235\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m    234\u001b[39m     args, kw = fix(args, kw, sig)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\IPython\\core\\formatters.py:282\u001b[39m, in \u001b[36mcatch_format_error\u001b[39m\u001b[34m(method, self, *args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     r = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\IPython\\core\\formatters.py:402\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[32m    404\u001b[39m method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170\u001b[39m, in \u001b[36mprint_figure\u001b[39m\u001b[34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[32m    168\u001b[39m     FigureCanvasBase(fig)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m data = bytes_io.getvalue()\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fmt == \u001b[33m'\u001b[39m\u001b[33msvg\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\matplotlib\\backend_bases.py:2158\u001b[39m, in \u001b[36mFigureCanvasBase.print_figure\u001b[39m\u001b[34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[39m\n\u001b[32m   2156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[32m   2157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches == \u001b[33m\"\u001b[39m\u001b[33mtight\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2158\u001b[39m         bbox_inches = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2159\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2160\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(layout_engine, ConstrainedLayoutEngine) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   2161\u001b[39m                 pad_inches == \u001b[33m\"\u001b[39m\u001b[33mlayout\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2162\u001b[39m             h_pad = layout_engine.get()[\u001b[33m\"\u001b[39m\u001b[33mh_pad\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\matplotlib\\figure.py:1848\u001b[39m, in \u001b[36mFigureBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, bbox_extra_artists)\u001b[39m\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ax.get_visible():\n\u001b[32m   1845\u001b[39m     \u001b[38;5;66;03m# some Axes don't take the bbox_extra_artists kwarg so we\u001b[39;00m\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# need this conditional....\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m         bbox = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1850\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1851\u001b[39m         bbox = ax.get_tightbbox(renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\matplotlib\\axes\\_base.py:4548\u001b[39m, in \u001b[36m_AxesBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[39m\n\u001b[32m   4546\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._axis_map.values():\n\u001b[32m   4547\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axison \u001b[38;5;129;01mand\u001b[39;00m axis.get_visible():\n\u001b[32m-> \u001b[39m\u001b[32m4548\u001b[39m         ba = \u001b[43mmartist\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_tightbbox_for_layout_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4549\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ba:\n\u001b[32m   4550\u001b[39m             bb.append(ba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\matplotlib\\artist.py:1402\u001b[39m, in \u001b[36m_get_tightbbox_for_layout_only\u001b[39m\u001b[34m(obj, *args, **kwargs)\u001b[39m\n\u001b[32m   1396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1397\u001b[39m \u001b[33;03mMatplotlib's `.Axes.get_tightbbox` and `.Axis.get_tightbbox` support a\u001b[39;00m\n\u001b[32m   1398\u001b[39m \u001b[33;03m*for_layout_only* kwarg; this helper tries to use the kwarg but skips it\u001b[39;00m\n\u001b[32m   1399\u001b[39m \u001b[33;03mwhen encountering third-party subclasses that do not support it.\u001b[39;00m\n\u001b[32m   1400\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfor_layout_only\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.get_tightbbox(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\matplotlib\\axis.py:1356\u001b[39m, in \u001b[36mAxis.get_tightbbox\u001b[39m\u001b[34m(self, renderer, for_layout_only)\u001b[39m\n\u001b[32m   1353\u001b[39m \u001b[38;5;28mself\u001b[39m._update_label_position(renderer)\n\u001b[32m   1355\u001b[39m \u001b[38;5;66;03m# go back to just this axis's tick labels\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m tlb1, tlb2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_ticklabel_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticks_to_draw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[38;5;28mself\u001b[39m._update_offset_text_position(tlb1, tlb2)\n\u001b[32m   1359\u001b[39m \u001b[38;5;28mself\u001b[39m.offsetText.set_text(\u001b[38;5;28mself\u001b[39m.major.formatter.get_offset())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\matplotlib\\axis.py:1332\u001b[39m, in \u001b[36mAxis._get_ticklabel_bboxes\u001b[39m\u001b[34m(self, ticks, renderer)\u001b[39m\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1331\u001b[39m     renderer = \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m)._get_renderer()\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ([\u001b[43mtick\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabel1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1333\u001b[39m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick.label1.get_visible()],\n\u001b[32m   1334\u001b[39m         [tick.label2.get_window_extent(renderer)\n\u001b[32m   1335\u001b[39m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick.label2.get_visible()])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\matplotlib\\text.py:971\u001b[39m, in \u001b[36mText.get_window_extent\u001b[39m\u001b[34m(self, renderer, dpi)\u001b[39m\n\u001b[32m    969\u001b[39m bbox, info, descent = \u001b[38;5;28mself\u001b[39m._get_layout(\u001b[38;5;28mself\u001b[39m._renderer)\n\u001b[32m    970\u001b[39m x, y = \u001b[38;5;28mself\u001b[39m.get_unitless_position()\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m x, y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m bbox = bbox.translated(x, y)\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bbox\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\matplotlib\\transforms.py:1495\u001b[39m, in \u001b[36mTransform.transform\u001b[39m\u001b[34m(self, values)\u001b[39m\n\u001b[32m   1492\u001b[39m values = values.reshape((-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.input_dims))\n\u001b[32m   1494\u001b[39m \u001b[38;5;66;03m# Transform the values\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1495\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform_non_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# Convert the result back to the shape of the input values.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ndim == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\matplotlib\\transforms.py:2410\u001b[39m, in \u001b[36mCompositeGenericTransform.transform_affine\u001b[39m\u001b[34m(self, values)\u001b[39m\n\u001b[32m   2408\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform_affine\u001b[39m(\u001b[38;5;28mself\u001b[39m, values):\n\u001b[32m   2409\u001b[39m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.transform(values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\venvs\\venv1\\Lib\\site-packages\\matplotlib\\transforms.py:2436\u001b[39m, in \u001b[36mCompositeGenericTransform.get_affine\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._b.get_affine()\n\u001b[32m   2435\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Affine2D(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_b\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_a\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# Train the Agent\n",
    "# -------------------------------------------\n",
    "\n",
    "\"\"\" Input Variables \"\"\"\n",
    "GRID_SIZE = 15\n",
    "NUM_REWARDS = 8\n",
    "NUM_ENEMIES = 0\n",
    "ENEMY_RANDOM_MOVE_RATIO = 0.6\n",
    "NUMBER_OF_EPISODES = 10000\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.95  # 0: only immediate reward matters ; 1.0: future rewards are just as important as immediate ones.\n",
    "EPSILON = 1.0   # initial value for weighting random over policy in taking actions\n",
    "EPSILON_MIN = 0.2\n",
    "EPSILON_DECAY = 0.9999  # multiplies random action chance with this factor after every training\n",
    "BATCH_SIZE = 8  # number of samples to take from the replay buffer for training\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Define the annealing parameters for beta (Prioritized Replay Buffer)\n",
    "BETA_START = 0.4  # Starting value for beta (usually smaller)\n",
    "BEAT_FRAMES = 10  # Number of frames after which beta will reach 1.0\n",
    "TOTAL_FRAMES = NUMBER_OF_EPISODES * 100  # Total frames in the training\n",
    "\n",
    "RESUME_TRAINING = False\n",
    "MODEL_PATH = r\"E:\\Git_repos\\RL_playground\\CollectAndAvoid\\trained_models\\cnn_models\\dqn_model_1.pth\"\n",
    "BUFFER_PATH = r\"E:\\Git_repos\\RL_playground\\CollectAndAvoid\\trained_models\\cnn_models\\buffer_1.pth\"\n",
    "\"\"\" END of Input Variables \"\"\"\n",
    "\n",
    "env = CollectAvoidEnv(grid_size=GRID_SIZE, num_rewards=NUM_REWARDS, num_enemies=NUM_ENEMIES, enemy_random_move_ratio=ENEMY_RANDOM_MOVE_RATIO)\n",
    "agent = DQNAgent(env.action_space.n, np.prod(env.observation_space.shape), lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON,\n",
    "                 epsilon_min=EPSILON_MIN, epsilon_decay=EPSILON_DECAY, batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE)\n",
    "\n",
    "start_episode = 0\n",
    "start_step = 0\n",
    "\n",
    "# Try loading an existing model\n",
    "if RESUME_TRAINING:\n",
    "    try:\n",
    "         start_episode, start_step = agent.load(MODEL_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved model found, starting from scratch.\")\n",
    "\n",
    "# Try loading an existing buffer\n",
    "if RESUME_TRAINING:\n",
    "    try:\n",
    "        agent.buffer.load(BUFFER_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved buffer found, starting with empty buffer.\")\n",
    "\n",
    "\n",
    "for episode in range(start_episode, NUMBER_OF_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    # Calculate beta for the current training step (frame_idx)\n",
    "    frame_idx = episode * 50 + step  # Adjust this according to your setup\n",
    "    beta = min(1.0, BETA_START + (BEAT_FRAMES - BETA_START) * frame_idx / TOTAL_FRAMES)\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)  # Get action from your agent\n",
    "        next_state, reward, done, _ = env.step(action, episode, step)\n",
    "        step += 1\n",
    "        agent.buffer.add((state, action, reward, next_state, float(done)))\n",
    "\n",
    "        print(f\"E{episode} S{step} | reward: {reward:.3f} | epsilon: {agent.epsilon:.3f} | beta: {beta:.3f}\")\n",
    "\n",
    "        agent.train(beta=beta)\n",
    "        # if episode > 2000:\n",
    "        #     time.sleep(0.1)\n",
    "        \n",
    "    # Every 10 episodes, update target network and save model and buffer\n",
    "    if episode % 10 == 0:\n",
    "        agent.update_target_network()\n",
    "        agent.save(MODEL_PATH, episode, step)\n",
    "        agent.buffer.save(BUFFER_PATH)\n",
    "        print(\"Model and Buffer saved.\")\n",
    "        \n",
    "    print(f\"Episode {episode + 1} finished\")\n",
    "\n",
    "# Save the final model and buffer after training is complete\n",
    "agent.save(MODEL_PATH, episode, step)\n",
    "agent.buffer.save(BUFFER_PATH)\n",
    "print(\"Training complete, model and buffer saved.\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b0d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
