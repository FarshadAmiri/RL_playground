{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da542328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from IPython.display import display, clear_output\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e135a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(episode_rewards, filename=\"trained_models/rewards_log.csv\"):\n",
    "    \"\"\"\n",
    "    Save episode rewards to a CSV file with total reward rounded to 1 decimal place.\n",
    "    If the file exists, it will append new values.\n",
    "    If not, it will create a new file with headers.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    # Round total reward to 1 decimal place\n",
    "    rounded_rewards = [[ep, round(rew, 0)] for ep, rew in episode_rewards]\n",
    "\n",
    "    with open(filename, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Episode\", \"TotalReward\"])\n",
    "        writer.writerows(rounded_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fa97993",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode, step = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f49357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Q-Network for DQN Agent\n",
    "# -----------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions, grid_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),  # [B, 32, H, W]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),              # [B, 64, H, W]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),              # [B, 64, H, W]\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Flatten after conv layers\n",
    "        self.fc_input_size = None  # Placeholder, will be set dynamically\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * grid_size * grid_size, 512),  # Update based on actual grid size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prioritized Replay Buffer\n",
    "# -----------------------------\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity (int): Maximum number of transitions to store.\n",
    "            alpha (float): How much prioritization is used \n",
    "                           (0 = no prioritization, 1 = full prioritization).\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []             # List to store experiences.\n",
    "        self.priorities = []         # List to store priorities.\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.pos = 0\n",
    "\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience to the buffer with maximum priority.\"\"\"\n",
    "        # If the buffer is not full, append the new experience;\n",
    "        # otherwise, replace the oldest one (circular buffer).\n",
    "        max_priority = max(self.priorities) if self.priorities else 1.0\n",
    "        max_priority += self.eps\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "            self.priorities.append(max_priority)\n",
    "        else:\n",
    "            self.buffer[self.pos] = experience\n",
    "            self.priorities[self.pos] = max_priority\n",
    "            self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"\n",
    "        Samples a batch of experiences with probabilities proportional to their priorities.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of samples to draw.\n",
    "            beta (float): Importance-sampling, from initial value increasing to 1.\n",
    "        \n",
    "        Returns:\n",
    "            samples: List of sampled experiences.\n",
    "            indices: The indices of the sampled experiences.\n",
    "            weights: Importance sampling weights for the batch.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        prios = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum() + self.eps\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max() + self.eps\n",
    "\n",
    "        return samples, indices, weights\n",
    "\n",
    "\n",
    "    def update_priorities(self, indices, new_priorities):\n",
    "        \"\"\"\n",
    "        Updates the priorities of sampled experiences.\n",
    "        \n",
    "        Args:\n",
    "            indices (list of int): The indices of the experiences to update.\n",
    "            new_priorities (list of float): The new priority for each corresponding experience.\n",
    "        \"\"\"\n",
    "        for idx, priority in zip(indices, new_priorities):\n",
    "            self.priorities[idx] = max(priority + self.eps, self.eps)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump((self.buffer, self.priorities, self.pos), f)\n",
    "        print(f\"Replay buffer saved to {filepath}\")\n",
    "\n",
    "    def load(self, filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            self.buffer, self.priorities, self.pos = pickle.load(f)\n",
    "        print(f\"Replay buffer loaded from {filepath}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DQN Agent\n",
    "# -----------------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_space, state_space, grid_size, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.1, \n",
    "                 epsilon_decay=0.995, batch_size=64, buffer_size=10000):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.step = 0  # <--- Track steps here\n",
    "\n",
    "        input_channels = 3  # Agent, rewards, enemies\n",
    "\n",
    "        self.q_network = QNetwork(input_channels, action_space, grid_size).to(self.device)\n",
    "        self.target_network = QNetwork(input_channels, action_space, grid_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.buffer = PrioritizedReplayBuffer(buffer_size)\n",
    "\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        \n",
    "        # Convert to shape [1, C, H, W] assuming 3-channel input (C=3)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # [1, 3, H, W]\n",
    "        q_values = self.q_network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "\n",
    "    def train(self, beta=0.4):\n",
    "        if self.buffer.size() < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample from PER buffer\n",
    "        samples, indices, weights = self.buffer.sample(self.batch_size, beta)\n",
    "        if not samples:\n",
    "            return  # Safety check\n",
    "\n",
    "        # Unpack and preprocess\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)  # [B, 3, H, W]\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Get Q-values for current states\n",
    "        q_values = self.q_network(states).gather(1, actions)  # [B, 1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Double DQN trick (optional):\n",
    "            # next_actions = self.q_network(next_states).argmax(1, keepdim=True)\n",
    "            # next_q_values = self.target_network(next_states).gather(1, next_actions)\n",
    "            \n",
    "            # Regular DQN:\n",
    "            next_q_values = self.target_network(next_states).max(1, keepdim=True)[0]  # [B, 1]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute TD errors and loss with importance-sampling weights\n",
    "        td_errors = q_values - target_q_values\n",
    "        loss = (weights * td_errors.pow(2)).mean()  # Weighted MSE loss\n",
    "\n",
    "        # Optimize Q-network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update buffer priorities\n",
    "        new_priorities = td_errors.abs().detach().cpu().numpy().flatten()\n",
    "        self.buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "        # Epsilon decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "    def save(self, filepath, episode, step):\n",
    "        torch.save({\n",
    "            'policy_net_state_dict': self.q_network.state_dict(),\n",
    "            'target_net_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'episode': episode,\n",
    "            'step': step,\n",
    "            'epsilon': self.epsilon\n",
    "        }, filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        episode = checkpoint.get('episode', 0)\n",
    "        step = checkpoint.get('step', 0)\n",
    "        self.epsilon = checkpoint.get('epsilon', 1.0)\n",
    "        print(f\"Loaded model from {filepath} | episode: {episode} | step: {step} | epsilon: {self.epsilon:.4f}\")\n",
    "        return episode, step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0069e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Collect & Avoid Environment (gym.Env subclass)\n",
    "# -------------------------------------------\n",
    "\n",
    "class CollectAvoidEnv(gym.Env):\n",
    "    def __init__(self, grid_size=15, num_rewards=5, num_enemies=3, enemy_random_move_ratio=0.5, max_steps=1000):\n",
    "        super(CollectAvoidEnv, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.num_rewards = num_rewards\n",
    "        self.num_enemies = num_enemies\n",
    "        self.enemy_random_move_ratio = enemy_random_move_ratio\n",
    "        self.reward_positions = []\n",
    "        self.enemy_positions = []\n",
    "        self.agent_pos = None\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.consecutive_stay_count = 0\n",
    "\n",
    "        # Action space: 5 discrete actions\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        # Observation space: 3-channel grid (agent, rewards, enemies)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=(3, self.grid_size, self.grid_size),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Plotting setup\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 6))\n",
    "        self.ax.set_xlim(0, self.grid_size - 1)\n",
    "        self.ax.set_ylim(0, self.grid_size - 1)\n",
    "        self.ax.set_xticks(range(self.grid_size))\n",
    "        self.ax.set_yticks(range(self.grid_size))\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        from math import sqrt\n",
    "\n",
    "        # Reset the step counter\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Reset consecutive stay counter\n",
    "        self.consecutive_stay_count = 0\n",
    "\n",
    "        # Randomly place rewards on the grid, avoiding overlap\n",
    "        self.reward_positions = [self._random_empty_cell([]) for _ in range(self.num_rewards)]\n",
    "\n",
    "        # Randomly place the agent on the grid, avoiding rewards\n",
    "        self.agent_pos = self._random_empty_cell(self.reward_positions)\n",
    "\n",
    "        # Calculate the minimum required distance between agent and each enemy\n",
    "        # 0.6 * grid_size * sqrt(2) is roughly 60% of the max diagonal distance\n",
    "        min_dist = 0.4 * self.grid_size * sqrt(2)\n",
    "\n",
    "        self.enemy_positions = []\n",
    "        for _ in range(self.num_enemies):\n",
    "            while True:\n",
    "                # Generate a candidate position for the enemy, avoiding overlaps\n",
    "                candidate = self._random_empty_cell(\n",
    "                    self.reward_positions + self.enemy_positions + [self.agent_pos]\n",
    "                )\n",
    "\n",
    "                # Compute Euclidean distance from the agent\n",
    "                dist = sqrt((candidate[0] - self.agent_pos[0])**2 +\n",
    "                            (candidate[1] - self.agent_pos[1])**2)\n",
    "\n",
    "                # Accept candidate only if it's far enough from the agent\n",
    "                if dist >= min_dist:\n",
    "                    self.enemy_positions.append(candidate)\n",
    "                    break\n",
    "\n",
    "        # Return the initial observation/state\n",
    "        return self._get_state()\n",
    "\n",
    "\n",
    "    def step(self, action, episode, step):\n",
    "        self.current_step += 1\n",
    "        prev_agent_pos = self.agent_pos\n",
    "        self.last_action = action\n",
    "\n",
    "        # Check if the agent stayed still\n",
    "        if action == 0:\n",
    "            self.consecutive_stay_count += 1\n",
    "        else:\n",
    "            self.consecutive_stay_count = 0\n",
    "\n",
    "        # Move agent\n",
    "        if action == 0:  # stay\n",
    "            new_pos = self.agent_pos\n",
    "        elif action == 1:  # up\n",
    "            new_pos = (max(self.agent_pos[0] - 1, 0), self.agent_pos[1])\n",
    "        elif action == 2:  # down\n",
    "            new_pos = (min(self.agent_pos[0] + 1, self.grid_size - 1), self.agent_pos[1])\n",
    "        elif action == 3:  # left\n",
    "            new_pos = (self.agent_pos[0], max(self.agent_pos[1] - 1, 0))\n",
    "        elif action == 4:  # right\n",
    "            new_pos = (self.agent_pos[0], min(self.agent_pos[1] + 1, self.grid_size - 1))\n",
    "\n",
    "        self.agent_pos = new_pos\n",
    "\n",
    "        # Move enemies\n",
    "        self._move_enemies()\n",
    "\n",
    "        # Compute reward and check if agent is caught by an enemy or has collected all rewards\n",
    "        reward = self._compute_reward(prev_agent_pos)\n",
    "        done = self.agent_pos in self.enemy_positions or len(self.reward_positions) == 0 or self.current_step >= self.max_steps # Terminate if caught by enemy or all rewards are collected\n",
    "\n",
    "        # Render environment\n",
    "        self.render(episode, step, reward)\n",
    "        return self._get_state(), reward, done, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        state = np.zeros((3, self.grid_size, self.grid_size), dtype=np.float32)\n",
    "\n",
    "        # Agent in channel 0\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "\n",
    "        # Rewards in channel 1\n",
    "        for r in self.reward_positions:\n",
    "            state[1, r[0], r[1]] = 1.0\n",
    "\n",
    "        # Enemies in channel 2\n",
    "        for e in self.enemy_positions:\n",
    "            state[2, e[0], e[1]] = 1.0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _compute_reward(self, prev_agent_pos):\n",
    "        reward = 0.0\n",
    "        reward_proximity_reward = 0.0\n",
    "        enemy_proximity_reward = 0.0\n",
    "\n",
    "        # Strong reward for collecting a reward\n",
    "        if self.agent_pos in self.reward_positions:\n",
    "            self.reward_positions.remove(self.agent_pos)\n",
    "            reward += 5.0  # Immediate collection reward\n",
    "\n",
    "        # Moderate reward for getting closer to rewards\n",
    "        for rx, ry in self.reward_positions:\n",
    "            dist = abs(self.agent_pos[0] - rx) + abs(self.agent_pos[1] - ry)\n",
    "            if dist == 1:\n",
    "                reward_proximity_reward += 0.6\n",
    "            elif dist == 2:\n",
    "                reward_proximity_reward += 0.4\n",
    "            elif dist == 3:\n",
    "                reward_proximity_reward += 0.2\n",
    "\n",
    "        reward += min(reward_proximity_reward, 1.0)\n",
    "\n",
    "        # Mild time penalty to encourage faster episode completion\n",
    "        reward -= 0.01\n",
    "\n",
    "        # Penalty for staying still more than 2 steps\n",
    "        if self.consecutive_stay_count > 1:\n",
    "            reward -= 0.1 * (self.consecutive_stay_count - 1)\n",
    "\n",
    "        # Penalty for hitting the wall (invalid move)\n",
    "        if self.agent_pos == prev_agent_pos:\n",
    "            x, y = prev_agent_pos\n",
    "            if (\n",
    "                (x == 0 and self.last_action == 1) or\n",
    "                (x == self.grid_size - 1 and self.last_action == 2) or\n",
    "                (y == 0 and self.last_action == 3) or\n",
    "                (y == self.grid_size - 1 and self.last_action == 4)\n",
    "            ):\n",
    "                reward -= 0.3\n",
    "\n",
    "        # Penalty for being close to enemies, reward for staying far\n",
    "        for ex, ey in self.enemy_positions:\n",
    "            edist = abs(self.agent_pos[0] - ex) + abs(self.agent_pos[1] - ey)\n",
    "            if edist == 1:\n",
    "                enemy_proximity_reward -= 1.0\n",
    "            elif edist == 2:\n",
    "                enemy_proximity_reward -= 0.5\n",
    "            elif edist == 3:\n",
    "                enemy_proximity_reward -= 0.2\n",
    "            elif edist >= self.grid_size // 3:\n",
    "                enemy_proximity_reward += 0.3  # modest reward for staying far\n",
    "\n",
    "        enemy_proximity_reward = max(-1.5, min(enemy_proximity_reward, 0.4))\n",
    "        reward += enemy_proximity_reward\n",
    "\n",
    "        # Small reward for increasing distance from enemy\n",
    "        if self.enemy_positions:\n",
    "            prev_avg = np.mean([abs(prev_agent_pos[0] - ex) + abs(prev_agent_pos[1] - ey) for ex, ey in self.enemy_positions])\n",
    "            curr_avg = np.mean([abs(self.agent_pos[0] - ex) + abs(self.agent_pos[1] - ey) for ex, ey in self.enemy_positions])\n",
    "            if curr_avg > prev_avg:\n",
    "                reward += 0.2\n",
    "            elif curr_avg < prev_avg:\n",
    "                reward -= 0.2\n",
    "\n",
    "        return round(reward, 2)  # Round for stable reward logging\n",
    "\n",
    "\n",
    "    def _move_enemies(self):\n",
    "        for i in range(len(self.enemy_positions)):\n",
    "            x, y = self.enemy_positions[i]\n",
    "            ax, ay = self.agent_pos\n",
    "\n",
    "            if random.random() < (1 - self.enemy_random_move_ratio):\n",
    "                if ax > x: x += 1\n",
    "                elif ax < x: x -= 1\n",
    "                if ay > y: y += 1\n",
    "                elif ay < y: y -= 1\n",
    "            else:\n",
    "                dx, dy = random.choice([(0, 1), (1, 0), (0, -1), (-1, 0)])\n",
    "                x = max(0, min(self.grid_size - 1, x + dx))\n",
    "                y = max(0, min(self.grid_size - 1, y + dy))\n",
    "\n",
    "            self.enemy_positions[i] = (x, y)\n",
    "\n",
    "    def _random_empty_cell(self, excluded_cells):\n",
    "        while True:\n",
    "            cell = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n",
    "            if cell not in excluded_cells:\n",
    "                return cell\n",
    "\n",
    "    def render(self, episode, step, reward=None):\n",
    "        self.ax.clear()\n",
    "        self.ax.set_xlim(0, self.grid_size - 1)\n",
    "        self.ax.set_ylim(0, self.grid_size - 1)\n",
    "        self.ax.set_xticks(range(self.grid_size))\n",
    "        self.ax.set_yticks(range(self.grid_size))\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        self.ax.plot(self.agent_pos[0], self.agent_pos[1], 'bo', markersize=10)\n",
    "        for r_pos in self.reward_positions:\n",
    "            self.ax.plot(r_pos[0], r_pos[1], 'go', markersize=10)\n",
    "        for e_pos in self.enemy_positions:\n",
    "            self.ax.plot(e_pos[0], e_pos[1], 'ro', markersize=10)\n",
    "\n",
    "        self.ax.text(0.5, self.grid_size - 1, f'Episode: {episode}, Step: {step}',\n",
    "                     horizontalalignment='left', verticalalignment='top', fontsize=12, color='black', weight='bold')\n",
    "\n",
    "        if reward is not None:\n",
    "            reward_color = 'green' if reward > 0 else 'red' if reward < 0 else 'black'\n",
    "            self.ax.text(0.5, self.grid_size - 2, f'Reward: {reward:.2f}',\n",
    "                         horizontalalignment='left', verticalalignment='top', fontsize=12, color=reward_color, weight='bold')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e20c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E40 S9 | reward: 0.190 | epsilon: 0.100 | beta: 0.402\n",
      "Replay buffer saved to E:\\Git_repos\\RL_playground\\CollectAndAvoid\\trained_models\\cnn_models\\buffer_dqn_cnn_1.pth\n",
      "Model and Buffer saved.\n",
      "[WARNING] Could not write to CSV: [Errno 13] Permission denied: 'E:\\\\Git_repos\\\\RL_playground\\\\CollectAndAvoid\\\\trained_models\\\\cnn_models\\\\log_dqn_cnn_1.csv'\n",
      "Retrying in 10 seconds...\n",
      "[WARNING] Could not write to CSV: [Errno 13] Permission denied: 'E:\\\\Git_repos\\\\RL_playground\\\\CollectAndAvoid\\\\trained_models\\\\cnn_models\\\\log_dqn_cnn_1.csv'\n",
      "Retrying in 10 seconds...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# Train the Agent\n",
    "# -------------------------------------------\n",
    "\n",
    "\"\"\" Input Variables \"\"\"\n",
    "GRID_SIZE = 15\n",
    "NUM_REWARDS = 5\n",
    "NUM_ENEMIES = 1\n",
    "ENEMY_RANDOM_MOVE_RATIO = 0.6  # otherwise, it will move towards the agent\n",
    "NUMBER_OF_EPISODES = 10000\n",
    "MAX_STEPS_PER_EPISODE= 150\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.95  # 0: only immediate reward matters ; 1.0: future rewards are just as important as immediate ones.\n",
    "EPSILON = 1.0   # initial value for weighting random over policy in taking actions\n",
    "EPSILON_MIN = 0.1\n",
    "EPSILON_DECAY = 0.997  # multiplies random action chance with this factor after every training\n",
    "BATCH_SIZE = 64  # number of samples to take from the replay buffer for training\n",
    "BUFFER_SIZE = 50000\n",
    "\n",
    "# Define the annealing parameters for beta (Prioritized Replay Buffer)\n",
    "BETA_START = 0.4  # Starting value for beta (usually smaller)\n",
    "BETA_FRAMES = NUMBER_OF_EPISODES * MAX_STEPS_PER_EPISODE  # Number of frames after which beta will reach 1.0\n",
    "\n",
    "RESUME_TRAINING = True\n",
    "MODEL_NAME = \"dqn_cnn_1\"\n",
    "\n",
    "# Home\n",
    "BASE_DIR = r\"E:\\Git_repos\\RL_playground\\CollectAndAvoid\"\n",
    "# Workpace\n",
    "# BASE_DIR = r\"D:\\Git_repos\\RL_playground\\CollectAndAvoid\"\n",
    "\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"trained_models\", \"cnn_models\", MODEL_NAME  + \".pth\")\n",
    "BUFFER_PATH = os.path.join(BASE_DIR, \"trained_models\", \"cnn_models\", \"buffer_\" + MODEL_NAME + \".pth\")\n",
    "CSV_LOG_PATH = os.path.join(BASE_DIR, \"trained_models\", \"cnn_models\", \"log_\" + MODEL_NAME + \".csv\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.join(BASE_DIR, \"trained_models\", \"cnn_models\"), exist_ok=True)  # Create models directory if it doesn't exist\n",
    "\n",
    "\"\"\" END of Input Variables \"\"\"\n",
    "\n",
    "env = CollectAvoidEnv(grid_size=GRID_SIZE + 1, num_rewards=NUM_REWARDS, num_enemies=NUM_ENEMIES, enemy_random_move_ratio=ENEMY_RANDOM_MOVE_RATIO, max_steps=MAX_STEPS_PER_EPISODE)\n",
    "agent = DQNAgent(env.action_space.n, np.prod(env.observation_space.shape), GRID_SIZE + 1, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON,\n",
    "                 epsilon_min=EPSILON_MIN, epsilon_decay=EPSILON_DECAY, batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE)\n",
    "\n",
    "start_episode = 0\n",
    "start_step = 0\n",
    "\n",
    "# Try loading an existing model\n",
    "if RESUME_TRAINING:\n",
    "    try:\n",
    "         start_episode, start_step = agent.load(MODEL_PATH)\n",
    "         print(f\"Model found Successfully. Training will resume from episode {start_episode} and step {start_step}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved model found, starting from scratch.\")\n",
    "\n",
    "# Try loading an existing buffer\n",
    "if RESUME_TRAINING:\n",
    "    try:\n",
    "        agent.buffer.load(BUFFER_PATH)\n",
    "        print(\"Buffer found Successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved buffer found, starting with empty buffer.\")\n",
    "\n",
    "# Let the user read whether model loaded successfully or the training prcoess is going to start from scratch\n",
    "time.sleep(5)\n",
    "\n",
    "episodes_total_rewards = []  # store total reward for each episode\n",
    "\n",
    "for episode in range(start_episode, NUMBER_OF_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    # Calculate beta for the current training step (frame_idx)\n",
    "    frame_idx = episode * MAX_STEPS_PER_EPISODE + step  # Adjust this according to your setup\n",
    "    beta = min(1.0, BETA_START + frame_idx * (1.0 - BETA_START) / BETA_FRAMES)\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)  # Get action from your agent\n",
    "        next_state, reward, done, _ = env.step(action, episode, step)\n",
    "        step += 1\n",
    "        total_reward += reward\n",
    "        agent.buffer.add((state, action, reward, next_state, float(done)))\n",
    "        print(f\"E{episode} S{step} | reward: {reward:.3f} | epsilon: {agent.epsilon:.3f} | beta: {beta:.3f}\")\n",
    "        agent.train(beta=beta)\n",
    "\n",
    "    episodes_total_rewards.append((episode, total_reward))\n",
    "\n",
    "    # Every 10 episodes, update target network and save model and buffer\n",
    "    if episode % 10 == 0 and episode > 0:\n",
    "        agent.update_target_network()\n",
    "        agent.save(MODEL_PATH, episode, step)\n",
    "        agent.buffer.save(BUFFER_PATH)\n",
    "        print(\"Model and Buffer saved.\")\n",
    "\n",
    "        # Save the 10 latest episodes rewards\n",
    "        if len(episodes_total_rewards) >= 10:\n",
    "            while True:\n",
    "                try:\n",
    "                    save_to_csv(episodes_total_rewards[-10:], filename=CSV_LOG_PATH)\n",
    "                    break  # Exit loop if save is successful\n",
    "                except (PermissionError, OSError) as e:\n",
    "                    print(f\"[WARNING] Could not write to CSV: {e}\")\n",
    "                    print(\"Retrying in 10 seconds...\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "    # Every 100 episodes, save a backup model and buffer\n",
    "    if episode % 100 == 0 and episode > 0:\n",
    "        agent.save(MODEL_PATH[:-4] + f\"_E{episode}_backup\" + \".pth\", episode, step)\n",
    "        agent.buffer.save(BUFFER_PATH[:-4] + f\"_E{episode}_backup\" + \".pth\")\n",
    "        print(\"Model and Buffer saved.\")\n",
    "    \n",
    "        \n",
    "    print(f\"Episode {episode + 1} finished\")\n",
    "\n",
    "# SavSSe the final model and buffer after training is complete\n",
    "agent.save(MODEL_PATH, episode, step)\n",
    "agent.buffer.save(BUFFER_PATH)\n",
    "print(\"Training complete, model and buffer saved.\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b6bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
