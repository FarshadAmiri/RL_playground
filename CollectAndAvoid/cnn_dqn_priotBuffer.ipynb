{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "da542328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from IPython.display import display, clear_output\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b4e135a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(episode_rewards, filename=\"trained_models/rewards_log.csv\"):\n",
    "    \"\"\"\n",
    "    Save episode rewards to a CSV file with total reward rounded to 1 decimal place.\n",
    "    If the file exists, it will append new values.\n",
    "    If not, it will create a new file with headers.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    # Round total reward to 1 decimal place\n",
    "    rounded_rewards = [[ep, round(rew, 0)] for ep, rew in episode_rewards]\n",
    "\n",
    "    with open(filename, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Episode\", \"TotalReward\"])\n",
    "        writer.writerows(rounded_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4fa97993",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode, step = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2f49357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Q-Network for DQN Agent\n",
    "# -----------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions, grid_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),  # [B, 32, H, W]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),              # [B, 64, H, W]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),              # [B, 64, H, W]\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Flatten after conv layers\n",
    "        self.fc_input_size = None  # Placeholder, will be set dynamically\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * grid_size * grid_size, 512),  # Update based on actual grid size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prioritized Replay Buffer\n",
    "# -----------------------------\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity (int): Maximum number of transitions to store.\n",
    "            alpha (float): How much prioritization is used \n",
    "                           (0 = no prioritization, 1 = full prioritization).\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []             # List to store experiences.\n",
    "        self.priorities = []         # List to store priorities.\n",
    "        self.alpha = alpha\n",
    "        self.pos = 0\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience to the buffer with maximum priority.\"\"\"\n",
    "        # If the buffer is not full, append the new experience;\n",
    "        # otherwise, replace the oldest one (circular buffer).\n",
    "        max_priority = max(self.priorities) if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "            self.priorities.append(max_priority)\n",
    "        else:\n",
    "            self.buffer[self.pos] = experience\n",
    "            self.priorities[self.pos] = max_priority\n",
    "            self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"\n",
    "        Samples a batch of experiences with probabilities proportional to their priorities.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of samples to draw.\n",
    "            beta (float): Importance-sampling, from initial value increasing to 1.\n",
    "        \n",
    "        Returns:\n",
    "            samples: List of sampled experiences.\n",
    "            indices: The indices of the sampled experiences.\n",
    "            weights: Importance sampling weights for the batch.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        prios = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = prios ** self.alpha\n",
    "        probs_sum = probs.sum()\n",
    "        if probs_sum == 0 or np.isnan(probs_sum):\n",
    "            probs = np.ones_like(probs) / len(probs)\n",
    "        else:\n",
    "            probs /= probs_sum\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()  # Normalize\n",
    "\n",
    "        return samples, indices, weights\n",
    "    \n",
    "\n",
    "    def update_priorities(self, indices, new_priorities):\n",
    "        \"\"\"\n",
    "        Updates the priorities of sampled experiences.\n",
    "        \n",
    "        Args:\n",
    "            indices (list of int): The indices of the experiences to update.\n",
    "            new_priorities (list of float): The new priority for each corresponding experience.\n",
    "        \"\"\"\n",
    "        for idx, priority in zip(indices, new_priorities):\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump((self.buffer, self.priorities, self.pos), f)\n",
    "        print(f\"Replay buffer saved to {filepath}\")\n",
    "\n",
    "    def load(self, filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            self.buffer, self.priorities, self.pos = pickle.load(f)\n",
    "        print(f\"Replay buffer loaded from {filepath}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DQN Agent\n",
    "# -----------------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_space, state_space, grid_size, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.1, \n",
    "                 epsilon_decay=0.995, batch_size=64, buffer_size=10000):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.step = 0  # <--- Track steps here\n",
    "\n",
    "        input_channels = 3  # Agent, rewards, enemies\n",
    "\n",
    "        self.q_network = QNetwork(input_channels, action_space, grid_size).to(self.device)\n",
    "        self.target_network = QNetwork(input_channels, action_space, grid_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.buffer = PrioritizedReplayBuffer(buffer_size)\n",
    "\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        \n",
    "        # Convert to shape [1, C, H, W] assuming 3-channel input (C=3)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # [1, 3, H, W]\n",
    "        q_values = self.q_network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, beta=0.4):\n",
    "        if self.buffer.size() < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch, indices, weights = self.buffer.sample(self.batch_size, beta)\n",
    "        if not batch:\n",
    "            return  # Safety check\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert batch to tensors\n",
    "        # Shape: [B, 3, H, W] assuming 3-channel grid input\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)  # [B, 3, H, W]\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)  # [B, 3, H, W]\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(1).to(self.device)  # [B, 1]\n",
    "\n",
    "        # Forward pass\n",
    "        q_values = self.q_network(states).gather(1, actions)  # [B, 1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1, keepdim=True)[0]  # [B, 1]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        td_errors = q_values - target_q_values\n",
    "        loss = (weights * td_errors.pow(2)).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update priorities\n",
    "        new_priorities = td_errors.abs().detach().cpu().numpy().flatten() + 1e-6\n",
    "        self.buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, filepath, episode, step):\n",
    "        torch.save({\n",
    "            'policy_net_state_dict': self.q_network.state_dict(),\n",
    "            'target_net_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'episode': episode,\n",
    "            'step': step,\n",
    "            'epsilon': self.epsilon\n",
    "        }, filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        episode = checkpoint.get('episode', 0)\n",
    "        step = checkpoint.get('step', 0)\n",
    "        self.epsilon = checkpoint.get('epsilon', 1.0)\n",
    "        print(f\"Loaded model from {filepath} | episode: {episode} | step: {step} | epsilon: {self.epsilon:.4f}\")\n",
    "        return episode, step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c0069e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Collect & Avoid Environment (gym.Env subclass)\n",
    "# -------------------------------------------\n",
    "\n",
    "class CollectAvoidEnv(gym.Env):\n",
    "    def __init__(self, grid_size=15, num_rewards=5, num_enemies=3, enemy_random_move_ratio=0.5, max_steps=1000):\n",
    "        super(CollectAvoidEnv, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.num_rewards = num_rewards\n",
    "        self.num_enemies = num_enemies\n",
    "        self.enemy_random_move_ratio = enemy_random_move_ratio\n",
    "        self.reward_positions = []\n",
    "        self.enemy_positions = []\n",
    "        self.agent_pos = None\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.consecutive_stay_count = 0\n",
    "\n",
    "        # Action space: 5 discrete actions\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        # Observation space: 3-channel grid (agent, rewards, enemies)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=(3, self.grid_size, self.grid_size),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Plotting setup\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 6))\n",
    "        self.ax.set_xlim(0, self.grid_size - 1)\n",
    "        self.ax.set_ylim(0, self.grid_size - 1)\n",
    "        self.ax.set_xticks(range(self.grid_size))\n",
    "        self.ax.set_yticks(range(self.grid_size))\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        from math import sqrt\n",
    "\n",
    "        # Reset the step counter\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Reset consecutive stay counter\n",
    "        self.consecutive_stay_count = 0\n",
    "\n",
    "        # Randomly place rewards on the grid, avoiding overlap\n",
    "        self.reward_positions = [self._random_empty_cell([]) for _ in range(self.num_rewards)]\n",
    "\n",
    "        # Randomly place the agent on the grid, avoiding rewards\n",
    "        self.agent_pos = self._random_empty_cell(self.reward_positions)\n",
    "\n",
    "        # Calculate the minimum required distance between agent and each enemy\n",
    "        # 0.6 * grid_size * sqrt(2) is roughly 60% of the max diagonal distance\n",
    "        min_dist = 0.4 * self.grid_size * sqrt(2)\n",
    "\n",
    "        self.enemy_positions = []\n",
    "        for _ in range(self.num_enemies):\n",
    "            while True:\n",
    "                # Generate a candidate position for the enemy, avoiding overlaps\n",
    "                candidate = self._random_empty_cell(\n",
    "                    self.reward_positions + self.enemy_positions + [self.agent_pos]\n",
    "                )\n",
    "\n",
    "                # Compute Euclidean distance from the agent\n",
    "                dist = sqrt((candidate[0] - self.agent_pos[0])**2 +\n",
    "                            (candidate[1] - self.agent_pos[1])**2)\n",
    "\n",
    "                # Accept candidate only if it's far enough from the agent\n",
    "                if dist >= min_dist:\n",
    "                    self.enemy_positions.append(candidate)\n",
    "                    break\n",
    "\n",
    "        # Return the initial observation/state\n",
    "        return self._get_state()\n",
    "\n",
    "\n",
    "    def step(self, action, episode, step):\n",
    "        self.current_step += 1\n",
    "        prev_agent_pos = self.agent_pos\n",
    "        self.last_action = action\n",
    "\n",
    "        # Check if the agent stayed still\n",
    "        if action == 0:\n",
    "            self.consecutive_stay_count += 1\n",
    "        else:\n",
    "            self.consecutive_stay_count = 0\n",
    "\n",
    "        # Move agent\n",
    "        if action == 0:  # stay\n",
    "            new_pos = self.agent_pos\n",
    "        elif action == 1:  # up\n",
    "            new_pos = (max(self.agent_pos[0] - 1, 0), self.agent_pos[1])\n",
    "        elif action == 2:  # down\n",
    "            new_pos = (min(self.agent_pos[0] + 1, self.grid_size - 1), self.agent_pos[1])\n",
    "        elif action == 3:  # left\n",
    "            new_pos = (self.agent_pos[0], max(self.agent_pos[1] - 1, 0))\n",
    "        elif action == 4:  # right\n",
    "            new_pos = (self.agent_pos[0], min(self.agent_pos[1] + 1, self.grid_size - 1))\n",
    "\n",
    "        self.agent_pos = new_pos\n",
    "\n",
    "        # Move enemies\n",
    "        self._move_enemies()\n",
    "\n",
    "        # Compute reward and check if agent is caught by an enemy or has collected all rewards\n",
    "        reward = self._compute_reward(prev_agent_pos)\n",
    "        done = self.agent_pos in self.enemy_positions or len(self.reward_positions) == 0 or self.current_step >= self.max_steps # Terminate if caught by enemy or all rewards are collected\n",
    "\n",
    "        # Render environment\n",
    "        self.render(episode, step, reward)\n",
    "        return self._get_state(), reward, done, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        state = np.zeros((3, self.grid_size, self.grid_size), dtype=np.float32)\n",
    "\n",
    "        # Agent in channel 0\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "\n",
    "        # Rewards in channel 1\n",
    "        for r in self.reward_positions:\n",
    "            state[1, r[0], r[1]] = 1.0\n",
    "\n",
    "        # Enemies in channel 2\n",
    "        for e in self.enemy_positions:\n",
    "            state[2, e[0], e[1]] = 1.0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _compute_reward(self, prev_agent_pos):\n",
    "        reward = 0.0\n",
    "        reward_proximity_reward = 0.0\n",
    "        enemy_proximity_reward = 0.0\n",
    "\n",
    "        # Reward for collecting a reward on grid\n",
    "        if self.agent_pos in self.reward_positions:\n",
    "            self.reward_positions.remove(self.agent_pos)\n",
    "            reward += 2.0\n",
    "\n",
    "        # Reward for being near remaining rewards\n",
    "        for rx, ry in self.reward_positions:\n",
    "            dist = abs(self.agent_pos[0] - rx) + abs(self.agent_pos[1] - ry)\n",
    "            if dist == 1: reward_proximity_reward += 0.3\n",
    "            elif dist == 2: reward_proximity_reward += 0.2\n",
    "            elif dist == 3: reward_proximity_reward += 0.1\n",
    "\n",
    "        reward_proximity_reward = min(reward_proximity_reward, 0.5)\n",
    "        reward += reward_proximity_reward\n",
    "\n",
    "        # Penalty for staying still more than 2 steps\n",
    "        if self.consecutive_stay_count > 1:\n",
    "            reward -= 0.25 * (self.consecutive_stay_count - 1)\n",
    "\n",
    "        # Penalty for hitting the wall and not moving (invalid move attempt)\n",
    "        if self.agent_pos == prev_agent_pos:\n",
    "            x, y = prev_agent_pos\n",
    "            # Check if at edge and tried to go beyond\n",
    "            if (\n",
    "                (x == 0 and self.last_action == 1) or     # Tried to move up at top border\n",
    "                (x == self.grid_size - 1 and self.last_action == 2) or  # Down at bottom\n",
    "                (y == 0 and self.last_action == 3) or     # Left at left border\n",
    "                (y == self.grid_size - 1 and self.last_action == 4)     # Right at right border\n",
    "            ):\n",
    "                reward -= 0.5  # Penalty for hitting the wall\n",
    "\n",
    "        # Penalty for being near enemies and Reward for being far from enemies\n",
    "        for ex, ey in self.enemy_positions:\n",
    "            edist = abs(self.agent_pos[0] - ex) + abs(self.agent_pos[1] - ey)\n",
    "            if edist == 1: enemy_proximity_reward -= 2\n",
    "            elif edist == 2: enemy_proximity_reward -= 1 # Penalty for being near enemies\n",
    "            elif edist == 3: enemy_proximity_reward -= 0.3 # Penalty for being near enemies \n",
    "            elif edist == 4: enemy_proximity_reward -= 0.2 # Penalty for being near enemies\n",
    "            elif edist >= int(self.grid_size / 3): enemy_proximity_reward += 0.4 # Reward for being far from enemies\n",
    "\n",
    "        # Clamp enemy_proximity_reward\n",
    "        enemy_proximity_reward = max(-2, min(enemy_proximity_reward, 0.4))\n",
    "\n",
    "        # Reward for increasing distance from enemies (compare current step distance from enemy to last step situation)\n",
    "        if self.enemy_positions:\n",
    "            prev_avg = np.mean([abs(prev_agent_pos[0] - ex) + abs(prev_agent_pos[1] - ey) for ex, ey in self.enemy_positions])\n",
    "            curr_avg = np.mean([abs(self.agent_pos[0] - ex) + abs(self.agent_pos[1] - ey) for ex, ey in self.enemy_positions])\n",
    "            if curr_avg > prev_avg: reward += 0.15\n",
    "            elif curr_avg < prev_avg: reward -= 0.15\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _move_enemies(self):\n",
    "        for i in range(len(self.enemy_positions)):\n",
    "            x, y = self.enemy_positions[i]\n",
    "            ax, ay = self.agent_pos\n",
    "\n",
    "            if random.random() < (1 - self.enemy_random_move_ratio):\n",
    "                if ax > x: x += 1\n",
    "                elif ax < x: x -= 1\n",
    "                if ay > y: y += 1\n",
    "                elif ay < y: y -= 1\n",
    "            else:\n",
    "                dx, dy = random.choice([(0, 1), (1, 0), (0, -1), (-1, 0)])\n",
    "                x = max(0, min(self.grid_size - 1, x + dx))\n",
    "                y = max(0, min(self.grid_size - 1, y + dy))\n",
    "\n",
    "            self.enemy_positions[i] = (x, y)\n",
    "\n",
    "    def _random_empty_cell(self, excluded_cells):\n",
    "        while True:\n",
    "            cell = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n",
    "            if cell not in excluded_cells:\n",
    "                return cell\n",
    "\n",
    "    def render(self, episode, step, reward=None):\n",
    "        self.ax.clear()\n",
    "        self.ax.set_xlim(0, self.grid_size - 1)\n",
    "        self.ax.set_ylim(0, self.grid_size - 1)\n",
    "        self.ax.set_xticks(range(self.grid_size))\n",
    "        self.ax.set_yticks(range(self.grid_size))\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        self.ax.plot(self.agent_pos[0], self.agent_pos[1], 'bo', markersize=10)\n",
    "        for r_pos in self.reward_positions:\n",
    "            self.ax.plot(r_pos[0], r_pos[1], 'go', markersize=10)\n",
    "        for e_pos in self.enemy_positions:\n",
    "            self.ax.plot(e_pos[0], e_pos[1], 'ro', markersize=10)\n",
    "\n",
    "        self.ax.text(0.5, self.grid_size - 1, f'Episode: {episode}, Step: {step}',\n",
    "                     horizontalalignment='left', verticalalignment='top', fontsize=12, color='black', weight='bold')\n",
    "\n",
    "        if reward is not None:\n",
    "            reward_color = 'green' if reward > 0 else 'red' if reward < 0 else 'black'\n",
    "            self.ax.text(0.5, self.grid_size - 2, f'Reward: {reward:.2f}',\n",
    "                         horizontalalignment='left', verticalalignment='top', fontsize=12, color=reward_color, weight='bold')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "60e20c06",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     82\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)  \u001b[38;5;66;03m# Get action from your agent\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     85\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[174], line 113\u001b[0m, in \u001b[0;36mCollectAvoidEnv.step\u001b[1;34m(self, action, episode, step)\u001b[0m\n\u001b[0;32m    110\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menemy_positions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_positions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;66;03m# Terminate if caught by enemy or all rewards are collected\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Render environment\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_state(), reward, done, {}\n",
      "Cell \u001b[1;32mIn[174], line 235\u001b[0m, in \u001b[0;36mCollectAvoidEnv.render\u001b[1;34m(self, episode, step, reward)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39max\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    232\u001b[0m                  horizontalalignment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, verticalalignment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, color\u001b[38;5;241m=\u001b[39mreward_color, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbold\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    234\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 235\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\IPython\\core\\formatters.py:182\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    180\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\IPython\\core\\formatters.py:226\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 226\u001b[0m     r \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\IPython\\core\\formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    345\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\IPython\\core\\pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 170\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(bytes_io, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\backend_bases.py:2193\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2192\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2193\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2194\u001b[0m             filename,\n\u001b[0;32m   2195\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2196\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2197\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2198\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2199\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2200\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\backend_bases.py:2043\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2041\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2042\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2043\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2046\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:497\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:445\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    441\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[0;32m    447\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    448\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:388\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[0;32m    387\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m draw(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[0;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\figure.py:3154\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3151\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3154\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[0;32m   3158\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\axes\\_base.py:3070\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[0;32m   3068\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[1;32m-> 3070\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3073\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\axis.py:1391\u001b[0m, in \u001b[0;36mAxis.draw\u001b[1;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1388\u001b[0m tlb1, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks_to_draw:\n\u001b[1;32m-> 1391\u001b[0m     \u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;66;03m# Shift label away from axes to avoid overlapping ticklabels.\u001b[39;00m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_label_position(renderer)\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\axis.py:295\u001b[0m, in \u001b[0;36mTick.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    292\u001b[0m renderer\u001b[38;5;241m.\u001b[39mopen_group(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, gid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gid())\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m artist \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgridline, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick1line, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick2line,\n\u001b[0;32m    294\u001b[0m                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel2]:\n\u001b[1;32m--> 295\u001b[0m     \u001b[43martist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\text.py:797\u001b[0m, in \u001b[0;36mText.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    793\u001b[0m             textrenderer\u001b[38;5;241m.\u001b[39mdraw_tex(gc, x, y, clean_line,\n\u001b[0;32m    794\u001b[0m                                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fontproperties, angle,\n\u001b[0;32m    795\u001b[0m                                   mtext\u001b[38;5;241m=\u001b[39mmtext)\n\u001b[0;32m    796\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 797\u001b[0m             \u001b[43mtextrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_line\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fontproperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mismath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mismath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[0;32m    802\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:194\u001b[0m, in \u001b[0;36mRendererAgg.draw_text\u001b[1;34m(self, gc, x, y, s, prop, angle, ismath, mtext)\u001b[0m\n\u001b[0;32m    191\u001b[0m font \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_font(prop)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# We pass '0' for angle here, since it will be rotated (in raster\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# space) in the following call to draw_text_image).\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m \u001b[43mfont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_hinting_flag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m font\u001b[38;5;241m.\u001b[39mdraw_glyphs_to_bitmap(\n\u001b[0;32m    196\u001b[0m     antialiased\u001b[38;5;241m=\u001b[39mgc\u001b[38;5;241m.\u001b[39mget_antialiased())\n\u001b[0;32m    197\u001b[0m d \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mget_descent() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m64.0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# Train the Agent\n",
    "# -------------------------------------------\n",
    "\n",
    "\"\"\" Input Variables \"\"\"\n",
    "GRID_SIZE = 15\n",
    "NUM_REWARDS = 3\n",
    "NUM_ENEMIES = 1\n",
    "ENEMY_RANDOM_MOVE_RATIO = 0.6\n",
    "NUMBER_OF_EPISODES = 10000\n",
    "MAX_STEPS_PER_EPISODE= 700\n",
    "\n",
    "LEARNING_RATE = 0.002\n",
    "GAMMA = 0.95  # 0: only immediate reward matters ; 1.0: future rewards are just as important as immediate ones.\n",
    "EPSILON = 1.0   # initial value for weighting random over policy in taking actions\n",
    "EPSILON_MIN = 0.2\n",
    "EPSILON_DECAY = 0.9999  # multiplies random action chance with this factor after every training\n",
    "BATCH_SIZE = 16  # number of samples to take from the replay buffer for training\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Define the annealing parameters for beta (Prioritized Replay Buffer)\n",
    "BETA_START = 0.4  # Starting value for beta (usually smaller)\n",
    "BEAT_FRAMES = 10  # Number of frames after which beta will reach 1.0\n",
    "TOTAL_FRAMES = NUMBER_OF_EPISODES * 100  # Total frames in the training\n",
    "\n",
    "RESUME_TRAINING = True\n",
    "MODEL_NAME = \"dqn_cnn_1\"\n",
    "\n",
    "# Home\n",
    "# BASE_DIR = r\"E:\\Git_repos\\RL_playground\\CollectAndAvoid\"\n",
    "# Workpace\n",
    "BASE_DIR = r\"D:\\Git_repos\\RL_playground\\CollectAndAvoid\"\n",
    "\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"trained_models\", \"cnn_models\", MODEL_NAME  + \".pth\")\n",
    "BUFFER_PATH = os.path.join(BASE_DIR, \"trained_models\", \"cnn_models\", \"buffer_\" + MODEL_NAME + \".pth\")\n",
    "CSV_LOG_PATH = os.path.join(BASE_DIR, \"trained_models\", \"cnn_models\", \"log_\" + MODEL_NAME + \".csv\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.join(BASE_DIR, \"trained_models\", \"cnn_models\"), exist_ok=True)  # Create models directory if it doesn't exist\n",
    "\n",
    "\"\"\" END of Input Variables \"\"\"\n",
    "\n",
    "env = CollectAvoidEnv(grid_size=GRID_SIZE + 1, num_rewards=NUM_REWARDS, num_enemies=NUM_ENEMIES, enemy_random_move_ratio=ENEMY_RANDOM_MOVE_RATIO, max_steps=MAX_STEPS_PER_EPISODE)\n",
    "agent = DQNAgent(env.action_space.n, np.prod(env.observation_space.shape), GRID_SIZE + 1, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON,\n",
    "                 epsilon_min=EPSILON_MIN, epsilon_decay=EPSILON_DECAY, batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE)\n",
    "\n",
    "start_episode = 0\n",
    "start_step = 0\n",
    "\n",
    "# Try loading an existing model\n",
    "if RESUME_TRAINING:\n",
    "    try:\n",
    "         start_episode, start_step = agent.load(MODEL_PATH)\n",
    "         print(f\"Model found Successfully. Training will resume from episode {start_episode} and step {start_step}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved model found, starting from scratch.\")\n",
    "\n",
    "# Try loading an existing buffer\n",
    "if RESUME_TRAINING:\n",
    "    try:\n",
    "        agent.buffer.load(BUFFER_PATH)\n",
    "        print(\"Buffer found Successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved buffer found, starting with empty buffer.\")\n",
    "\n",
    "# Let the user read whether model loaded successfully or the training prcoess is going to start from scratch\n",
    "time.sleep(5)\n",
    "\n",
    "episodes_total_rewards = []  # store total reward for each episode\n",
    "\n",
    "for episode in range(start_episode, NUMBER_OF_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    # Calculate beta for the current training step (frame_idx)\n",
    "    frame_idx = episode * 50 + step  # Adjust this according to your setup\n",
    "    beta = min(1.0, BETA_START + (BEAT_FRAMES - BETA_START) * frame_idx / TOTAL_FRAMES)\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)  # Get action from your agent\n",
    "        next_state, reward, done, _ = env.step(action, episode, step)\n",
    "        step += 1\n",
    "        total_reward += reward\n",
    "        agent.buffer.add((state, action, reward, next_state, float(done)))\n",
    "        print(f\"E{episode} S{step} | reward: {reward:.3f} | epsilon: {agent.epsilon:.3f} | beta: {beta:.3f}\")\n",
    "        agent.train(beta=beta)\n",
    "\n",
    "    episodes_total_rewards.append((episode, total_reward))\n",
    "\n",
    "    # Every 10 episodes, update target network and save model and buffer\n",
    "    if episode % 10 == 0 and episode > 0:\n",
    "        agent.update_target_network()\n",
    "        agent.save(MODEL_PATH, episode, step)\n",
    "        agent.buffer.save(BUFFER_PATH)\n",
    "        print(\"Model and Buffer saved.\")\n",
    "\n",
    "        # Save the 10 latest episodes rewards\n",
    "        if len(episodes_total_rewards) >= 10:\n",
    "            while True:\n",
    "                try:\n",
    "                    save_to_csv(episodes_total_rewards[-10:], filename=CSV_LOG_PATH)\n",
    "                    break  # Exit loop if save is successful\n",
    "                except (PermissionError, OSError) as e:\n",
    "                    print(f\"[WARNING] Could not write to CSV: {e}\")\n",
    "                    print(\"Retrying in 10 seconds...\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "    # Every 100 episodes, save a backup model and buffer\n",
    "    if episode % 15 == 0 and episode > 0:\n",
    "        agent.save(MODEL_PATH[:-4] + f\"_E{episode}_backup\" + \".pth\", episode, step)\n",
    "        agent.buffer.save(BUFFER_PATH[:-4] + f\"_E{episode}_backup\" + \".pth\")\n",
    "        print(\"Model and Buffer saved.\")\n",
    "    \n",
    "        \n",
    "    print(f\"Episode {episode + 1} finished\")\n",
    "\n",
    "# SavSSe the final model and buffer after training is complete\n",
    "agent.save(MODEL_PATH, episode, step)\n",
    "agent.buffer.save(BUFFER_PATH)\n",
    "print(\"Training complete, model and buffer saved.\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b6bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
